{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-13 11:13:56.430757: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow-datasets in /Users/knut/anaconda3/envs/experiments/lib/python3.9/site-packages (4.9.3)\n",
      "Requirement already satisfied: absl-py in /Users/knut/anaconda3/envs/experiments/lib/python3.9/site-packages (from tensorflow-datasets) (2.1.0)\n",
      "Requirement already satisfied: array-record in /Users/knut/anaconda3/envs/experiments/lib/python3.9/site-packages (from tensorflow-datasets) (0.4.1)\n",
      "Requirement already satisfied: click in /Users/knut/anaconda3/envs/experiments/lib/python3.9/site-packages (from tensorflow-datasets) (8.1.7)\n",
      "Requirement already satisfied: dm-tree in /Users/knut/anaconda3/envs/experiments/lib/python3.9/site-packages (from tensorflow-datasets) (0.1.8)\n",
      "Requirement already satisfied: etils>=0.9.0 in /Users/knut/anaconda3/envs/experiments/lib/python3.9/site-packages (from etils[enp,epath,etree]>=0.9.0->tensorflow-datasets) (1.5.2)\n",
      "Requirement already satisfied: numpy in /Users/knut/anaconda3/envs/experiments/lib/python3.9/site-packages (from tensorflow-datasets) (1.26.4)\n",
      "Requirement already satisfied: promise in /Users/knut/anaconda3/envs/experiments/lib/python3.9/site-packages (from tensorflow-datasets) (2.3)\n",
      "Requirement already satisfied: protobuf>=3.20 in /Users/knut/anaconda3/envs/experiments/lib/python3.9/site-packages (from tensorflow-datasets) (3.20.3)\n",
      "Requirement already satisfied: psutil in /Users/knut/anaconda3/envs/experiments/lib/python3.9/site-packages (from tensorflow-datasets) (6.1.0)\n",
      "Requirement already satisfied: requests>=2.19.0 in /Users/knut/anaconda3/envs/experiments/lib/python3.9/site-packages (from tensorflow-datasets) (2.32.3)\n",
      "Requirement already satisfied: tensorflow-metadata in /Users/knut/anaconda3/envs/experiments/lib/python3.9/site-packages (from tensorflow-datasets) (1.16.1)\n",
      "Requirement already satisfied: termcolor in /Users/knut/anaconda3/envs/experiments/lib/python3.9/site-packages (from tensorflow-datasets) (2.5.0)\n",
      "Requirement already satisfied: toml in /Users/knut/anaconda3/envs/experiments/lib/python3.9/site-packages (from tensorflow-datasets) (0.10.2)\n",
      "Requirement already satisfied: tqdm in /Users/knut/anaconda3/envs/experiments/lib/python3.9/site-packages (from tensorflow-datasets) (4.67.1)\n",
      "Requirement already satisfied: wrapt in /Users/knut/anaconda3/envs/experiments/lib/python3.9/site-packages (from tensorflow-datasets) (1.16.0)\n",
      "Requirement already satisfied: fsspec in /Users/knut/anaconda3/envs/experiments/lib/python3.9/site-packages (from etils[enp,epath,etree]>=0.9.0->tensorflow-datasets) (2024.10.0)\n",
      "Requirement already satisfied: importlib_resources in /Users/knut/anaconda3/envs/experiments/lib/python3.9/site-packages (from etils[enp,epath,etree]>=0.9.0->tensorflow-datasets) (6.4.5)\n",
      "Requirement already satisfied: typing_extensions in /Users/knut/anaconda3/envs/experiments/lib/python3.9/site-packages (from etils[enp,epath,etree]>=0.9.0->tensorflow-datasets) (4.12.2)\n",
      "Requirement already satisfied: zipp in /Users/knut/anaconda3/envs/experiments/lib/python3.9/site-packages (from etils[enp,epath,etree]>=0.9.0->tensorflow-datasets) (3.20.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/knut/anaconda3/envs/experiments/lib/python3.9/site-packages (from requests>=2.19.0->tensorflow-datasets) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/knut/anaconda3/envs/experiments/lib/python3.9/site-packages (from requests>=2.19.0->tensorflow-datasets) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/knut/anaconda3/envs/experiments/lib/python3.9/site-packages (from requests>=2.19.0->tensorflow-datasets) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/knut/anaconda3/envs/experiments/lib/python3.9/site-packages (from requests>=2.19.0->tensorflow-datasets) (2024.8.30)\n",
      "Requirement already satisfied: six in /Users/knut/anaconda3/envs/experiments/lib/python3.9/site-packages (from promise->tensorflow-datasets) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/knut/anaconda3/envs/experiments/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.16.2\n"
     ]
    }
   ],
   "source": [
    "# import libraries\n",
    "#try:\n",
    "  # %tensorflow_version only exists in Colab.\n",
    "#  !pip install tf-nightly\n",
    "#except Exception:\n",
    "#  pass\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from tensorflow import keras\n",
    "%pip install tensorflow-datasets\n",
    "import tensorflow_datasets as tfds\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from keras.api.models import Sequential\n",
    "from keras.api.layers import Dense, Dropout\n",
    "from keras.api.callbacks import EarlyStopping\n",
    "from keras.api.layers import TextVectorization\n",
    "\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get data files\n",
    "#!wget https://cdn.freecodecamp.org/project-data/sms/train-data.tsv\n",
    "#!wget https://cdn.freecodecamp.org/project-data/sms/valid-data.tsv\n",
    "\n",
    "train_file_path = \"train-data.tsv\"\n",
    "test_file_path = \"valid-data.tsv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#You should create a function called predict_message that takes a message string as an argument and returns a list. \n",
    "# The first element in the list should be a number between zero and one that indicates \n",
    "# the likeliness of \"ham\" (0) or \"spam\" (1). The second element in the list should be the word \"ham\" or \"spam\", \n",
    "# depending on which is most likely.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  class                                            message\n",
      "0   ham  ahhhh...just woken up!had a bad dream about u ...\n",
      "1   ham                           you can never do nothing\n",
      "2   ham  now u sound like manky scouse boy steve,like! ...\n",
      "3   ham  mum say we wan to go then go... then she can s...\n",
      "4   ham  never y lei... i v lazy... got wat? dat day ü ...\n",
      "  class                                            message\n",
      "0   ham  i am in hospital da. . i will return home in e...\n",
      "1   ham         not much, just some textin'. how bout you?\n",
      "2   ham  i probably won't eat at all today. i think i'm...\n",
      "3   ham  don‘t give a flying monkeys wot they think and...\n",
      "4   ham                                who are you seeing?\n"
     ]
    }
   ],
   "source": [
    "header = [\"class\", \"message\"]\n",
    "\n",
    "train_file = pd.read_table(train_file_path, names=header) #See https://www.geeksforgeeks.org/how-to-convert-tab-separated-file-into-a-dataframe-using-python/\n",
    "\n",
    "test_file = pd.read_table(test_file_path, names=header)\n",
    "\n",
    "print(train_file.head())\n",
    "print(test_file.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#In case there should be duplicates I will drop them \n",
    "train_file = train_file.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_features length 3935\n",
      "train_labels length 3935\n",
      "test_features length 1392\n",
      "test_labels length 1392\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0        ham\n",
       "1        ham\n",
       "2        ham\n",
       "3        ham\n",
       "4        ham\n",
       "        ... \n",
       "4174     ham\n",
       "4175     ham\n",
       "4176    spam\n",
       "4177    spam\n",
       "4178     ham\n",
       "Name: class, Length: 3935, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_features = train_file.copy()\n",
    "train_labels = train_features.pop(\"class\") #https://www.codecademy.com/resources/docs/pandas/dataframe/pop\n",
    "\n",
    "test_features = test_file.copy()\n",
    "test_labels = test_features.pop(\"class\")\n",
    "\n",
    "print(\"train_features length \" + str(train_features.shape[0]))\n",
    "print(\"train_labels length \" + str(train_labels.shape[0]))\n",
    "print(\"test_features length \" + str(test_features.shape[0]))\n",
    "print(\"test_labels length \" + str(test_labels.shape[0]))\n",
    "\n",
    "train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "910\n",
      "482\n"
     ]
    }
   ],
   "source": [
    "#Finding the longest messages in order to later train a RNN network\n",
    "print(train_features.message.str.len().max()) #https://blog.finxter.com/python-find-longest-string-in-a-dataframe-column/\n",
    "print(test_features.message.str.len().max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Transform Labels to categorical \n",
    "\n",
    "# Convert categorical data to numerical data using cat.codes\n",
    "#train_labels = train_labels.replace(\"ham\", \"1\").replace(\"spam\", \"0\").astype(int)\n",
    "#test_labels = test_labels.replace(\"ham\", \"1\").replace(\"spam\", \"0\").astype(int)\n",
    "\n",
    "\n",
    "#train_labels = train_labels.astype(int)\n",
    "\n",
    "  #see https://saturncloud.io/blog/how-to-convert-categorical-data-to-numerical-data-with-pandas/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_x_np length 3935\n",
      "train_y_np length 3935\n",
      "test_x_np length 1392\n",
      "test_y_np length 1392\n",
      "910\n",
      "482\n"
     ]
    }
   ],
   "source": [
    "#Eventually I found no better way to get my tensorflows model work...(2.16.2). It was always complaining over a match of datatypes\n",
    "\n",
    "train_x_np = np.array(train_file[\"message\"])\n",
    "train_y_np = np.array(pd.factorize(train_file[\"class\"])[0]) #See https://www.geeksforgeeks.org/python-pandas-factorize/\n",
    "\n",
    "test_x_np = np.array(test_file[\"message\"])\n",
    "test_y_np = np.array(pd.factorize(test_file[\"class\"])[0]) #I am accessing the returned encoded values only as I am not interested in the unique set\n",
    "\n",
    "\n",
    "print(\"train_x_np length \" + str(train_x_np.shape[0]))\n",
    "print(\"train_y_np length \" + str(train_y_np.shape[0]))\n",
    "print(\"test_x_np length \" + str(test_x_np.shape[0]))\n",
    "print(\"test_y_np length \" + str(test_y_np.shape[0]))\n",
    "\n",
    "\n",
    "\n",
    "#Finding the longest messages in order to later train a RNN network\n",
    "print(len(max(train_x_np, key=len))) #See https://blog.finxter.com/how-to-find-the-longest-string-in-a-numpy-array/\n",
    "print(len(max(test_x_np, key=len))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#See https://www.tensorflow.org/tutorials/text/text_classification_rnn\n",
    "encoder = TextVectorization(\n",
    "    output_mode='int',\n",
    "    max_tokens=1000,\n",
    "    output_sequence_length=1000,\n",
    ")\n",
    "\n",
    "encoder.adapt(train_x_np)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see https://www.tensorflow.org/text/tutorials/text_classification_rnn#create_the_model\n",
    "model = keras.Sequential([\n",
    "    encoder,\n",
    "    keras.layers.Embedding(\n",
    "        input_dim = len(encoder.get_vocabulary()),\n",
    "        output_dim = 64,\n",
    "        mask_zero=True,\n",
    "    ),\n",
    "    keras.layers.Bidirectional(keras.layers.LSTM(64, return_sequences=True)),\n",
    "    keras.layers.Bidirectional(keras.layers.LSTM(32)),\n",
    "    keras.layers.Dense(64, activation=\"relu\"),\n",
    "    keras.layers.Dense(1, activation=\"sigmoid\") #This is to get return values between 0 and 1\n",
    "])\n",
    "\n",
    "\n",
    "model.compile(\n",
    "    loss=keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "    optimizer=keras.optimizers.Adam(1e-4), #Frankly speaking I have no clue what the meaning/effect of this one is, just took it from the tensorflow tutorial...\n",
    "    metrics=[\"accuracy\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/knut/anaconda3/envs/experiments/lib/python3.9/site-packages/keras/src/backend/tensorflow/nn.py:707: UserWarning: \"`binary_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Sigmoid activation and thus does not represent logits. Was this intended?\n",
      "  output, from_logits = _get_logits(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m260s\u001b[0m 2s/step - accuracy: 0.8550 - loss: 0.6447 - val_accuracy: 0.8657 - val_loss: 0.4533\n",
      "Epoch 2/5\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m198s\u001b[0m 2s/step - accuracy: 0.8728 - loss: 0.3890\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-14 09:51:56.730867: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "\t [[{{node IteratorGetNext}}]]\n",
      "/Users/knut/anaconda3/envs/experiments/lib/python3.9/contextlib.py:137: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self.gen.throw(typ, value, traceback)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m211s\u001b[0m 2s/step - accuracy: 0.9547 - loss: 0.1644 - val_accuracy: 0.9648 - val_loss: 0.1099\n",
      "Epoch 4/5\n",
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m176s\u001b[0m 1s/step - accuracy: 0.9761 - loss: 0.0939\n",
      "Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-14 09:58:23.623425: W tensorflow/core/framework/local_rendezvous.cc:404] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n",
      "\t [[{{node IteratorGetNext}}]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m123/123\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m164s\u001b[0m 1s/step - accuracy: 0.9873 - loss: 0.0609 - val_accuracy: 0.9770 - val_loss: 0.0759\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    x=train_x_np.astype(object),\n",
    "    y=train_y_np,\n",
    "    validation_data = [test_x_np.astype(object), test_y_np],\n",
    "    validation_steps=20,\n",
    "    epochs=5,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 254ms/step\n",
      "[0.013175111, 'ham']\n",
      "[0.013175111, 'ham']\n"
     ]
    }
   ],
   "source": [
    "# function to predict messages based on model\n",
    "# (should return list containing prediction and label, ex. [0.008318834938108921, 'ham'])\n",
    "def predict_message(pred_text):\n",
    "\n",
    "  pred = model.predict(np.array([pred_text]).astype(object))\n",
    "  prob = pred[0][0]\n",
    "  \n",
    "  return_value = [prob, \"ham\" if prob <0.4 else \"spam\"] #See https://stackoverflow.com/questions/2802726/putting-a-simple-if-then-else-statement-on-one-line\n",
    "\n",
    "  print(return_value)\n",
    "\n",
    "  return return_value \n",
    "\n",
    "pred_text = \"how are you doing today?\"\n",
    "\n",
    "prediction = predict_message(pred_text)\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 193ms/step\n",
      "[0.013175111, 'ham']\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 437ms/step\n",
      "[0.41724747, 'spam']\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 345ms/step\n",
      "[0.000136912, 'ham']\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 280ms/step\n",
      "[0.9156565, 'spam']\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 265ms/step\n",
      "[0.8307691, 'spam']\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 272ms/step\n",
      "[0.0021458922, 'ham']\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 234ms/step\n",
      "[0.00031310323, 'ham']\n",
      "You passed the challenge. Great job!\n"
     ]
    }
   ],
   "source": [
    "# Run this cell to test your function and model. Do not modify contents.\n",
    "def test_predictions():\n",
    "  test_messages = [\"how are you doing today\",\n",
    "                   \"sale today! to stop texts call 98912460324\",\n",
    "                   \"i dont want to go. can we try it a different day? available sat\",\n",
    "                   \"our new mobile video service is live. just install on your phone to start watching.\",\n",
    "                   \"you have won £1000 cash! call to claim your prize.\",\n",
    "                   \"i'll bring it tomorrow. don't forget the milk.\",\n",
    "                   \"wow, is your arm alright. that happened to me one time too\"\n",
    "                  ]\n",
    "\n",
    "  test_answers = [\"ham\", \"spam\", \"ham\", \"spam\", \"spam\", \"ham\", \"ham\"]\n",
    "  passed = True\n",
    "\n",
    "  for msg, ans in zip(test_messages, test_answers):\n",
    "    prediction = predict_message(msg)\n",
    "    if prediction[1] != ans:\n",
    "      passed = False\n",
    "\n",
    "  if passed:\n",
    "    print(\"You passed the challenge. Great job!\")\n",
    "  else:\n",
    "    print(\"You haven't passed yet. Keep trying.\")\n",
    "\n",
    "test_predictions()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "experiments",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
